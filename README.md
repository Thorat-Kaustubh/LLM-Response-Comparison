<!-- PROJECT HEADER -->
<div align="center">

# ğŸ” **LLM Response Comparison Dashboard**

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://YOUR_STREAMLIT_APP_URL_HERE)
[![Python Version](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Open Source Love](https://badges.frapsoft.com/os/v2/open-source.svg?v=103)](https://github.com/YOUR_USERNAME)

_A visual and analytical dashboard to compare responses from generative AI models under two prompt configurations._

</div>

---

## ğŸŒˆ **Overview**

The **LLM Response Comparison Dashboard** provides a simple and powerful way to **visually compare, analyze, and quantify** how a generative model (Googleâ€™s `gemini-2.5-flash-preview-09-2025`) behaves under:

1. **User Prompt Only**
2. **System Prompt + User Prompt**

Itâ€™s built for **AI researchers, NLP developers, and prompt engineers** who want to measure â€” not guess â€” how prompts influence model performance.

<p align="center">
  <img src="image1.jpg"  width="600"><br><br>
  <img src="image2.jpg" width="600">
</p>

---

## âš¡ **Core Features**

âœ¨ **Side-by-Side Comparison**  
> Instantly see differences between plain user prompts and engineered (system + user) prompts.  

ğŸ“Š **Automated Metrics**  
> Compare **token count**, **character length**, and **finish reason** automatically.  

ğŸ§  **Smart Insights**  
> Generates a plain-English verdict on which response is more detailed, concise, or token-efficient.  

ğŸ§© **Clean Architecture**  
> Decoupled backend (`app_backend.py`) for logic and frontend (`app_frontend.py`) for UI.  

ğŸ” **Raw API Viewer**  
> Inspect raw JSON responses from the API for debugging and transparency.

---

## ğŸ’¡ **Why This Project?**

Prompt engineering is powerful â€” but often **hard to prove**.

This project provides a visual and analytical way to:
- âœ… **Quantify** the impact of a system prompt  
- ğŸ” **Debug** prompt behavior for tone, structure, and coherence  
- ğŸ“ˆ **Demonstrate** measurable improvements to teams or clients  

> _â€œIf you can measure it, you can improve it.â€_ â€” Peter Drucker

---

## ğŸ§  **Tech Stack**

| Layer | Technology |
|--------|-------------|
| **Frontend** | [Streamlit](https://streamlit.io/) |
| **Backend** | [Python](https://www.python.org/) |
| **Model** | `gemini-2.5-flash-preview-09-2025` |
| **API Calls** | `requests` |
| **Data Handling** | `pandas` |
| **Visualization** | Streamlit components & Markdown UI |

---

## âš™ï¸ **Setup & Installation**

### ğŸª„ Step 1 â€” Clone the Repository
```bash
git clone https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git
cd YOUR_REPO_NAME


### ğŸ§© Step 2 â€” Install Dependencies
pip install -r requirements.txt

###ğŸ”‘ Step 3 â€” Configure API Key
Create .streamlit/secrets.toml and add your Gemini API key:

# .streamlit/secrets.toml
GEMINI_API_KEY = "YOUR_API_KEY_GOES_HERE"

###â–¶ï¸ Step 4 â€” Run the App
streamlit run app_frontend.py

Then open your browser and visit ğŸ‘‰ http://localhost:8501

## ğŸ¯ **How to Use**

Enter User Prompt:
Example â€” â€œExplain quantum entanglement.â€

Enter System Prompt:
Example â€” â€œYou are a physics professor. Explain simply.â€

Click â€œCompare Responses.â€

Analyze Results:

View both responses side-by-side

Review token and character metrics

Read the automated insights to see which version performs better


ğŸ·ï¸ License

This project is licensed under the MIT License â€” see the LICENSE file for details.
